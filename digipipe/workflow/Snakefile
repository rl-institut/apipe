"""
Snakemake file for digipipe
"""

from snakemake.utils import min_version
min_version("6.0")

from digipipe.scripts.config import load_dataset_configs
from digipipe.scripts.data_io import extract_zipfile, copy_files, clean_folder
from digipipe.store.utils import get_abs_dataset_path
from digipipe.config import GLOBAL_CONFIG

config = GLOBAL_CONFIG
config.update(load_dataset_configs())

# include store modules
include: "../store/preprocessed/module.smk"
include: "../store/datasets/module.smk"

# ===== RULES =====

rule all:
    input:
        forests=rules.datasets_osm_forest_extract_tags.output,
        muns=rules.datasets_bkg_vg250_muns_region_create.output,
        districts=rules.datasets_bkg_vg250_districts_region_create.output,
        region=rules.datasets_bkg_vg250_region_create.output,
        wind=rules.datasets_bnetza_mastr_wind_region_create.output,
        pv_ground=rules.datasets_bnetza_mastr_pv_ground_region_create.output,
        pv_roof=rules.datasets_bnetza_mastr_pv_roof_region_create.output,
        biomass=rules.datasets_bnetza_mastr_biomass_region_create.output,
        hydro=rules.datasets_bnetza_mastr_hydro_region_create.output,
        combustion=rules.datasets_bnetza_mastr_combustion_region_create.output,
        mastr_names=rules.datasets_bnetza_mastr_captions_create.output,
        gsgk=rules.datasets_bnetza_mastr_gsgk_region_create.output,
        storage=rules.datasets_bnetza_mastr_storage_region_create.output,
        population=rules.datasets_population_create.output
    # run:
    #     print("CONFIG MAIN:")
    #     print(config)
    #     print(workflow.basedir)

rule clean:
    """
    Remove all output and temporary files.
    """
    params:
        preprocessed=expand(
            get_abs_dataset_path("preprocessed", "{name}", data_dir=True) / "*",
            name=config.get("store")["preprocessed"].keys()
        ),
        datasets=expand(
            get_abs_dataset_path("datasets", "{name}", data_dir=True) / "*",
            name=config.get("store")["datasets"].keys()
        )
    shell:
        """
        rm -f {params.preprocessed}
        rm -f {params.datasets}
        """

rule download_raw_zip:
    """
    Downloads zipfile from Wolke containing the raw data and stores it in 'store/temp'
    """
    output:
        raw_zip = "store/temp/store_raw.zip",
    shell:
        """
        wget -O {input.raw_zip} https://wolke.rl-institut.de/s/w8WKwXT3f9ZzZQJ/download --retry-connrefused --waitretry=1 --read-timeout=15 --timeout=10 -t 0
        """

rule update_raw:
    """
    Extracts the downloaded zipfile, copies containing folders to 'store/raw' and cleans up 'store/temp' afterwards
    """
    input:
        raw_zip="store/temp/store_raw.zip"
    params:
        temp_dir="store/temp",
        raw_dir="store/raw",
        temp_raw_dir="store/temp/store/raw"
    run:
        extract_zipfile(input.raw_zip, params.temp_dir)
        copy_files(params.temp_raw_dir, params.raw_dir)
        clean_folder(params.temp_dir)
