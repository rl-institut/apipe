# Workflow

## Run

To run the pipeline, go to Digipipe's root `digipipe/` or to
`digipipe/workflow/` and type

    snakemake -j<NUMBER_OF_CPU_CORES>

while `NUMBER_OF_CPU_CORES` is the number of CPU cores to be used for the
pipeline execution.  You can also make a dry-run (see what snakemake would do
but without actually really doing anything) by typing

    snakemake -n

To clean all produced data, use

    snakemake -j1 -p clean

## Pipeline visualization / DAG

The entire pipeline can be visualized as directed acyclic graph (DAG) with

    snakemake --dag | dot -Tsvg > dag_rules_full.svg

As the full graph is too packed with information and therefore hardly to grasp,
consider to show only certain parts by disabling some target files in the `all`
rule. Also, a simple rule graph (the one shown above) can be created using

    snakemake --rulegraph | dot -Tsvg > dag_rules_simple.svg

To create a graph showing the file dependencies, type

    snakemake --filegraph | dot -Tsvg > dag_files.svg

The graphs also provide information on the completed (solid lines) and pending
(dashed lines) processing steps. For further details see
[Snakemake CLI docs](https://snakemake.readthedocs.io/en/stable/executing/cli.html).

## Snakefiles and config

- The global workflow is defined in the main
  [Snakefile](../workflow/Snakefile).
- It includes the module Snakefiles from the data store located at
  - [store/preprocessed/module.smk](../store/preprocessed/module.smk) and
  - [store/datasets/module.smk](../store/datasets/module.smk)
- In each of these modules, the rules as well as the config from the contained
  datasets are imported.
  